{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Python Basics with Numpy\n",
    "\n",
    "This assignment gives you a brief introduction on how to code some useful functions in Python 3. \n",
    "\n",
    "**After this assignment you will:**\n",
    "- Be able to use iPython Notebooks (create Python files with extention .IPYNB)\n",
    "- Be able to use numpy functions and numpy matrix/vector operations such as np.sum, np.dot, np.multiply, np.maximum,  np.exp, np.log, and np.reshape.\n",
    "- Understand the concept of \"broadcasting\"\n",
    "- Be able to vectorize code and compute L1 and L2 loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About iPython Notebooks ##\n",
    "\n",
    "Project Jupyter proposed in 2014 by Fernando Pérez. \n",
    "Reference to the 3 core programming languages supported Julia, Python, R. \n",
    "Open-source software, open-standards,  language agnostic \n",
    "web-based interactive computational environment for creating notebook documents. \n",
    "Ordered list of cells which can contain code, text (Markdown: using Markup language – html, latex), maths, plots. \n",
    "\n",
    "iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this course. After writing your code, you can run the cell by clicking on \"Run\" in the upper bar of the notebook. \n",
    "\n",
    "To undo text entry press \"CTRL+Z\". \n",
    "\n",
    "**Exercise**: Set test to `\"Hello World\"` in the cell below. Add a new cell and print test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Hello World\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"test: \"+ test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**:\n",
    "test: Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Building basic functions with numpy ##\n",
    "\n",
    "Numpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). In this exercise you will learn several numpy functions such as np.exp, np.log, np.reshape.\n",
    "\n",
    "### 1.1 - sigmoid function, np.exp() ###\n",
    "\n",
    "To refer to a function belonging to a specific package you could call it using package_name.function().\n",
    "\n",
    "Before using np.exp(), you will use math.exp() to implement the sigmoid function. You will then see why np.exp() is preferable to math.exp().\n",
    "\n",
    "**Note**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ is also known as the logistic function. It is a non-linear function used both in Machine Learning (e.g. Logistic Regression method) and Deep Learning (DL).\n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:400px;height:200px;\">\n",
    "\n",
    "**Exercise**: Complete the function *basic_sigmoid(x)* that returns the sigmoid of a real number x. Use **math.exp(x)** for the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- a scalar value\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    s= ?\n",
    "   \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function basic_sigmoid to compute sigmoid of 3. \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: 0.9525741268224334\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs of the functions in \"math\" library are scalar real numbers. \n",
    "In Machine Learning (ML) we mostly use matrices and vectors. This is why numpy is more useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call function basic_sigmoid to compute the sigmoid of vector x. \n",
    "# It will not work, why ?\n",
    "x = [1, 2, 3]\n",
    "\n",
    "#?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $ x = (x_1, x_2, ..., x_n)$ is a row vector then $np.exp(x)$ will apply the exponential function to every element of x. \n",
    "The output will be also a vector: $np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access numpy functions by writing np.function_name() instead of numpy.function_name()\n",
    "import numpy as np  \n",
    "\n",
    "# Use np.array() do create the array [1, 2, 3].\n",
    "#x = ?\n",
    "\n",
    "# Apply np.exp() to x and print the result.\n",
    "\n",
    "np.exp?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If x is a vector (array), Python operations (e.g. $s = x + 3$; $s = \\frac{1}{x}$ will output s as a vector of the same size as x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n",
      "[1.         0.5        0.33333333]\n"
     ]
    }
   ],
   "source": [
    "# Use np.array() do create the array [1, 2, 3].\n",
    "x = \n",
    "\n",
    "#Print the result of operation (x+3)\n",
    "\n",
    "#Print the result of operation (1/x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need more info on a numpy function, look at [the official documentation](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
    "\n",
    "You can also  write `np.exp?` to get quick access to the documentation.\n",
    "\n",
    "Data structures used in numpy to represent vectors, matrices are called numpy arrays. \n",
    "$$ \\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\n",
    "    x_2  \\\\\n",
    "    ...  \\\\\n",
    "    x_n  \\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\n",
    "    ...  \\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\n",
    "\\end{pmatrix}\\tag{1} $$\n",
    "\n",
    "**Exercise**: Complete *sigmoid* function using numpy. \n",
    "x can now be a scalar, vector, matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments: x - a scalar or numpy array of any size\n",
    "\n",
    "    Return:  s - sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    s = ?\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the array [1, 2, 3] \n",
    "x = ?\n",
    "# Call function sigmoid to compute the sigmoid of array x.\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:  array([ 0.73105858,  0.88079708,  0.95257413])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid gradient\n",
    "\n",
    "Gradients are very important to train ML models.\n",
    "\n",
    "**Exercise**: Implement function *sigmoid_derivative()* to compute the gradient of the sigmoid function with respect to its input x. \n",
    "The formula is: $sigmoid\\_derivative(x) = sigmoid(x) (1 - sigmoid(x))\\tag{2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of \n",
    "    the sigmoid function with respect to x.\n",
    "     \n",
    "    Arguments:  x - a scalar number or numpy array\n",
    "    Return:    ds - the computed gradient.\n",
    "    \"\"\"\n",
    "        \n",
    "    ds = ?\n",
    "        \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the array [1, 2, 3] \n",
    "x = ?\n",
    "# Compute the gradients of x  \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: [ 0.19661193  0.10499359  0.04517666]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Reshaping arrays ###\n",
    "\n",
    "Two useful numpy functions are [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) and [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- X.shape is used to get the shape (dimension) of a matrix/vector X. \n",
    "- X.reshape(...) is used to reshape X into some other dimension. \n",
    "\n",
    "For example, in computer science, an RGB image is represented as a 3D array of shape $(length, height, depth = 3)$. However, when the image is an input of a ML algorithm you may need to convert it to a column vector of shape $(length*height*3, 1)$. In other words, you \"unroll\", or reshape, the 3D array into a 1D vector.\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**Exercise**: Implement `image2vector()` that takes an input of shape (length, height, depth) and returns a vector of shape (length\\*height\\*depth, 1). \n",
    "\n",
    "- Extract the image dimensions with `image.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument: image, a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:    v, a column vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "  \n",
    "    v = ?\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "image = np.array([[[ 0.67826139,  0.29380381],  \n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "#Compute the shape (dimension) of image.\n",
    "\n",
    "#Call function image2vector and check how the shape of image changed\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: vector column with shape (18,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Normalizing rows and Broadcasting\n",
    "\n",
    "Useful technique in ML is data normalization. The  optimization algorithms converge faster after normalization. There are different types of normalization, here we apply $ \\frac{x}{\\| x\\|} $ (divide each row of x by its norm 2).\n",
    "\n",
    "For example, if $$x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4 \\\\\n",
    "\\end{bmatrix}\\tag{3}$$ then $$\\| x\\| = np.linalg.norm(x, axis = 1, keepdims = True) = \\begin{bmatrix}\n",
    "    5 \\\\\n",
    "    \\sqrt{56} \\\\\n",
    "\\end{bmatrix}\\tag{4} $$\n",
    "\n",
    " norm 2 ($\\| x\\|$) is computed as: \n",
    " \n",
    " $\\sqrt(0^2+3^2+4^2) =5$ (norm of row 1)\n",
    " \n",
    " $\\sqrt(2^2+6^2+4^2) =\\sqrt(56)$ (norm of row 2)   \n",
    " \n",
    "then\n",
    "\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}} \\\\\n",
    "\\end{bmatrix}\\tag{5}$$ \n",
    "\n",
    "The norm of each row of $x\\_normalized $ is 1, known also as unit length.  \n",
    "\n",
    "**Exercise**: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length. \n",
    "\n",
    "**Broadcasting**\n",
    "\n",
    "You can divide matrices of different sizes: x.shape=(2,3) ,  x_norm.shape=(2,1), and **x/x_norm** works fine due to **python broadcasting**.\n",
    "\n",
    "Python will copy x_norm 3 times and will apply for each column of x. \n",
    "\n",
    "An important concept in numpy is \"**broadcasting**\". It is very useful for performing math operations between arrays of different shapes. \n",
    "\n",
    "For more inf on broadcasting, read the official [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).\n",
    "\n",
    "**General Principle of Broadcasting**: \n",
    "\n",
    "A is (m,n) matrix, B is (1,n) matrix  => Python will copy m times B and will do element-wise operations (+, -, *, /) between A and B. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    Normalizes each row of matrix x to have unit length.\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- Normalized (by row) numpy matrix. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute x_norm as norm 2 of x. \n",
    "    # Use np.linalg.norm(..., axis = ..., keepdims = True)\n",
    "    # In Python axis=0 is the vertical axis (columns) \n",
    "    # axis=1 is the horizontal axis (rows)\n",
    "    \n",
    "    x_norm = \n",
    "    \n",
    "    # Divide x by its norm to get the normalized (by row) matrix\n",
    "    𝑥_normalized = ?\n",
    "   \n",
    "    return 𝑥_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "\n",
    "#Apply normalizeRows to x\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    " [[ 0.          0.6         0.8       ]\n",
    " \n",
    " [ 0.13736056  0.82416338  0.54944226]]\n",
    "\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2) Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WHENEVER POSSIBLE AVOID EXPLICIT FOR-LOOPs !!!**\n",
    "\n",
    "In ML, we deal with large datasets. Hence, a non-computationally-optimal function can result in a model that takes ages to run. \n",
    "The vectorized implementation is more efficient. \n",
    "\n",
    "**Note** that `np.dot()` performs a matrix-matrix or matrix-vector multiplication. This is different from `np.multiply()` and`*` operator which perform an element-wise multiplication ( equivalent to  `.*` in Matlab/Octave). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the difference between the following implementations of \n",
    "# dot product.\n",
    "\n",
    "import time\n",
    "#generate matrix a with random values \n",
    "a=np.random.rand(1000000) \n",
    "#What is the shape of a\n",
    "?\n",
    "# #generate matrix b with the same shape as a  \n",
    "b= ?\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "start_time = time.perf_counter()\n",
    "# call np.dot to implement matrix multiplication between a and b\n",
    "c= ?\n",
    "\n",
    "timer = time.perf_counter() - start_time\n",
    "print(timer)\n",
    "print(c)\n",
    "\n",
    "### FOR LOOP DOT PRODUCT OF VECTORS ###\n",
    "c=0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i]*b[i]\n",
    "toc=time.time()\n",
    "print ( \" for loop = \" + str(1000*(toc-tic)) + \"ms\")\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Implement L1 and L2 loss functions\n",
    "\n",
    "**Exercise**: Implement numpy vectorized version to compute L1 loss function. Use abs(x) to compute absolute value of x. \n",
    "\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}\\tag{6}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted values/labels)\n",
    "    y -- vector of size m (true value/labels)\n",
    "    \n",
    "    Returns:loss - value of L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = ?\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "#Call function L1 to compute L1 loss \n",
    "\n",
    " ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:    *L1* = 1.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Implement the numpy vectorized version to compute L2 loss function: $\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted value/labels)\n",
    "    y -- vector of size m (true value/labels)\n",
    "    \n",
    "    Returns: loss - value of L2 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "     loss = ?\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "#Compute L2 loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: *L2* = 0.43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on python/numpy vectors\n",
    "\n",
    "Broadcasting operations have both strengths and weaknesses. Strength because it lets you get a lot done even with just a single line of code. But there's also weakness because with broadcasting sometimes it's possible you can introduce very subtle strange looking bugs. For example, if you take a column vector and add it to a row vector, you would expect an error message. But you might actually get back a matrix as a sum of a row vector and a column vector. \n",
    "\n",
    "Don't use data structures where the shape is (n, ), i.e. rank 1 array. \n",
    "Instead, every time you create an array, make it explicitly a column or a row vector.  \n",
    "\n",
    "Check this with the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(5)  # rank 1 array \n",
    "\n",
    "# Compute the shape of a\n",
    "?\n",
    "\n",
    "# Print a and a_transpose (a.T) and see there is no difference between them\n",
    "?\n",
    "\n",
    "\n",
    "#You can reshape rank 1 array to have two dimensions\n",
    "#a=a.reshape((5,1))\n",
    "#print(a.shape)\n",
    "\n",
    "#or better use \n",
    "#b = np.random.randn(5,1) \n",
    "#print(b.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
